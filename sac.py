# -*- coding: utf-8 -*-
"""SAC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XqxhAD4mnMuXSTEe0D0X0cDNVzZunm6B
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class SoftQNetwork(nn.Module):
  def __init__(self, num_inputs, num_actions, hidden_size = 256, init_w=3e-3):
    super(SoftQNetwork, self).__init__()
    self.w = num_inputs[0]
    self.h = num_inputs[1]
    self.in_channel = num_inputs[2]
    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    self.conv1 = nn.Conv2d(self.in_channel, 256, 3)
    self.conv2 = nn.Conv2d(256, 512, 3)
    self.linear3 = nn.Linear(32514049, 1)

    self.linear3.weight.data.uniform_(-init_w, init_w)
    self.linear3.bias.data.uniform_(-init_w, init_w)

  def forward(self, state, action):

    x = F.relu(self.conv1(state))
    x = F.relu(self.conv2(x))
    x = torch.flatten(x)
    x = torch.cat((x, action.unsqueeze(0))).to(self.device)
    x = self.linear3(x)
    return x

  def sample_batch(self, states, actions):
    q_values = []

    for state, action in zip(states, actions):
      q_values.append(self.forward(state, action))

    return torch.tensor(q_values, requires_grad=True).to(self.device)

from torch.distributions import Normal

class GaussianPolicy(nn.Module):
  def __init__(self, num_inputs, num_actions, hidden_size=256, init_w=3e-3, log_std_min=20, log_std_max=2):
    super(GaussianPolicy, self).__init__()

    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    self.in_channel = num_inputs[-1]

    self.log_std_min = log_std_min
    self.log_std_max = log_std_max

    self.conv1 = nn.Conv2d(self.in_channel, 256, 3)
    self.conv2 = nn.Conv2d(256, 512, 3)

    #mean of the gaussian
    self.mean_linear = nn.Linear(32514048, num_actions)
    self.mean_linear.weight.data.uniform_(-init_w, init_w)
    self.mean_linear.bias.data.uniform_(-init_w, init_w)
    
    #network head for log(covariance) of the gaussian distribution
    self.log_std_linear = nn.Linear(32514048, num_actions)
    self.log_std_linear.weight.data.uniform_(-init_w, init_w)
    self.log_std_linear.bias.data.uniform_(-init_w, init_w)

  
  def forward(self, state):
    x = F.relu(self.conv1(state))
    x = F.relu(self.conv2(x))
    x = torch.flatten(x)

    mean = self.mean_linear(x)
    log_std = self.log_std_linear(x)
    log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)

    return mean, log_std

  def sample_batch(self, states, epsilon=1e-6):
    actions = []
    log_pis = []

    for image in states:
      mean, log_std = self.forward(image)
      std = log_std.exp()

      normal = Normal(mean, std)
      z = normal.rsample()
      # action = torch.tanh(z)
      action = z

      log_pi = (normal.log_prob(z) - torch.log(1-(torch.tanh(z)).pow(2) + epsilon)).sum()

      actions.append(action)
      log_pis.append(log_pi)
    
    actions = torch.tensor(actions, requires_grad=True).to(self.device)
    log_pis = torch.tensor(log_pis, requires_grad=True).to(self.device)

    return actions, log_pis

class ValueNetwork(nn.Module):
  def __init__(self, input_dim, output_dim, init_w=3e-3):
    super(ValueNetwork, self).__init__()

    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    self.in_channel = input_dim[-1]
    self.conv1 = nn.Conv2d(self.in_channel, 256, 3)
    self.conv2 = nn.Conv2d(256, 512, 3)
    self.fc3 = nn.Linear(32514048, output_dim)


    self.fc3.weight.data.uniform_(-init_w, init_w)
    self.fc3.bias.data.uniform_(-init_w, init_w)

  def forward(self, state):
    x = F.relu(self.conv1(state))
    x = F.relu(self.conv2(x))
    x = torch.flatten(x)
    x = self.fc3(x)

    return x

  def sample_batch(self, states):
    values = []

    for state in states:
      values.append(self.forward(state))

    return torch.tensor(values, requires_grad=True).to(self.device)

import random
import numpy as np
from collections import deque

class BasicBuffer:
  def __init__(self, max_size):
    self.max_size = max_size
    self.buffer = deque(maxlen=max_size)

  def push(self, state, action, reward, next_state, done):
    experience = (state, action, np.array([reward]), next_state, done)
    self.buffer.append(experience)

  def sample(self, batch_size):
    state_batch = []
    action_batch = []
    reward_batch = []
    next_state_batch = []
    done_batch = []

    batch = random.sample(self.buffer, batch_size)

    for experience in batch:
      state, action, reward, next_state, done = experience
      state_batch.append(state)
      action_batch.append(action)
      reward_batch.append(reward)
      next_state_batch.append(next_state)
      done_batch.append(done)

    return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)

  def sample_sequence(self, batch_size):
    state_batch = []
    action_batch = []
    reward_batch = []
    next_state_batch = []
    done_batch = []

    min_start = len(self.buffer) - batch_size
    start = np.random.randing(0, min_start)

    for sample in range(start, start + batch_size):
      state, action, reward, next_state, done = self.buffer[sample]
      state_batch.append(state)
      action_batch.append(action)
      reward_batch.append(reward)
      next_state_batch.append(next_state)
      done_batch.append(done)

    return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)

  def __len__(self):
    return len(self.buffer)

import torch.optim as optim

class SACAgent:

  def __init__(self, env, gamma, tau, v_lr, q_lr, policy_lr, buffer_maxlen, load_path=False):
    self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    self.env = env
    self.action_range = [env.action_space.low, env.action_space.high]
    self.obs_dim = env.observation_space.shape
    self.action_dim = env.action_space.shape[0]

    #hyperparameters
    self.gamma = gamma
    self.tau = tau
    self.update_step = 0
    self.delay_step = 2

    #initialize network
    self.value_net = ValueNetwork(self.obs_dim, 1).to(self.device)
    self.target_value_net = ValueNetwork(self.obs_dim, 1).to(self.device)
    self.q_net1 = SoftQNetwork(self.obs_dim, self.action_dim).to(self.device)
    self.q_net2 = SoftQNetwork(self.obs_dim, self.action_dim).to(self.device)
    self.policy_net = GaussianPolicy(self.obs_dim, self.action_dim).to(self.device)

    if load_path:
      checkpoint = torch.load(load_path)
      self.value_net.load_state_dict(checkpoint['value_net'])
      self.targe_value_net.load_state_dict(checkpoint['target_value_net'])
      self.q_net1.load_state_dict(checkpoint['q_net1'])
      self.q_net2.load_state_dict(checkpoint['q_net2'])
      self.policy_net.load_state_dict(checkpoint['policy_net'])

    #copy params to target param
    for target_param, param in zip(self.target_value_net.parameters(), self.value_net.parameters()):
      target_param.data.copy_(param)

    #initialize optimizers
    self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=v_lr)
    self.q1_optimizer = optim.Adam(self.q_net1.parameters(), lr=q_lr)
    self.q2_optimizer = optim.Adam(self.q_net2.parameters(), lr=q_lr)
    self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)

    self.replay_buffer = BasicBuffer(buffer_maxlen)

  def get_action(self, state):
    state = torch.FloatTensor(state).to(self.device)
    mean, log_std = self.policy_net.forward(state)
    std = log_std.exp()

    normal = Normal(mean,std)
    z = normal.sample()
    # action = torch.tanh(z)
    action = z
    action = action.cpu().detach().squeeze(0).numpy()

    return action

  def rescale_action(self, action):
    return action * (self.action_range[1] - self.action_range[0]) / 2.0 + (self.action_range[1] + self.action_range[0]) / 2.0

  def update(self, batch_size):
    states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
    states = torch.stack(states).to(self.device)
    actions = torch.FloatTensor(np.array(actions)).to(self.device)
    rewards = torch.FloatTensor(rewards).to(self.device)
    next_states = torch.stack(next_states).to(self.device)
    dones = torch.FloatTensor(dones).to(self.device)
    dones = dones.view(dones.size(0), -1)

    next_actions, next_log_pi = self.policy_net.sample_batch(next_states)
    next_q1 = self.q_net1.sample_batch(next_states, next_actions)
    next_q2 = self.q_net2.sample_batch(next_states, next_actions)
    next_v = self.target_value_net.sample_batch(next_states)

    #value loss
    next_v_target = torch.min(next_q1, next_q2) - next_log_pi
    curr_v = self.value_net.sample_batch(states)
    v_loss = F.mse_loss(curr_v, next_v_target.detach())

    #q loss
    curr_q1 = self.q_net1.sample_batch(states, actions)
    curr_q2 = self.q_net2.sample_batch(states, actions)
    expected_q = rewards + (1-dones) * self.gamma * next_v
    q1_loss = F.mse_loss(curr_q1, expected_q.detach())
    q2_loss = F.mse_loss(curr_q2, expected_q.detach())

    #update value network and q networks
    self.value_optimizer.zero_grad()
    v_loss.backward()
    self.value_optimizer.step()

    self.q1_optimizer.zero_grad()
    q1_loss.backward()
    self.q1_optimizer.step()

    self.q2_optimizer.zero_grad()
    q2_loss.backward()
    self.q2_optimizer.step()

    #delayed update for policy net and target value nets
    if self.update_step % self.delay_step == 0:
      new_actions, log_pi = self.policy_net.sample_batch(states)
      min_q = torch.min(self.q_net1.sample_batch(states, new_actions), self.q_net2.sample_batch(states, new_actions))
      policy_loss = (log_pi - min_q).mean()

      self.policy_optimizer.zero_grad()
      policy_loss.backward()
      self.policy_optimizer.step()

      #target networks
      for target_param, param in zip(self.target_value_net.parameters(), self.value_net.parameters()):
        target_param.data.copy_(self.tau * param + (1-self.tau) * target_param)
      
    self.update_step += 1
  
  def save_agent(self, path):
    torch.save({
      'value_net': self.value_net,
      'target_value_net': self.target_value_net,
      'q_net1': self.q_net1,
      'q_net2': self.q_net2,
      'policy_net': self.policy_net,
    }, path)

import gym
from gym import spaces
from scipy.stats import entropy

class VoteEnv(gym.Env):
  def __init__(self, dataset_file_path):

    self.action_space = spaces.Box(low=0, high=10, shape=(1,), dtype=np.float32)
    self.observation_space = spaces.Box(low=0, high=255, shape=(256,256,3), dtype=np.uint8)
    self.reward_range = (-200, 200)
    self.max_step = 30
    self.curr_step = 1
    self.dataset = dataset_file_path
    self.batch = self._sample(self.max_step+1)

    self._last = False
    self.reset()

  def reset(self):
    self.curr_step = 1

    #sample new training episode images
    batch_size = self.max_step
    self.batch = self._sample(batch_size)

    return 

  def _sample(self, batch_size):
    batch = []
    with h5py.File(FilePath, 'r') as f:
      max_size = f['input_image'].shape[0]
      sample_idx = random.sample(range(max_size), batch_size)

      for i in sample_idx:
        batch.append(f['input_image'][i])

    return batch

  def _next_observation(self):
    #currently returns random image from the dataset
    obs = torch.FloatTensor(self.batch[self.curr_step-1])
    return torch.movedim(obs, -1,0)

  def _calculate_reward(self, actions):
    #just an inverse of entropy
    return 1/torch.var(actions)


  def step(self, actions):
    self.curr_step += 1

    reward = self._calculate_reward(torch.tensor(actions))
    obs = self._next_observation()
    done = self.curr_step == self.max_step

    return obs, reward, done, {}

from tqdm import tqdm
from matplotlib import pyplot as plt
def mini_batch_train(env, agents, max_episodes, max_steps=30, batch_size=30):
    episode_rewards = []

    for episode in range(max_episodes):
        # state = env.reset()
        episode_reward = 0
        
        highest_image=False
        lowest_image=False

        lowest_score=False
        highest_score=False
        
        
        for step in tqdm(range(max_steps)):
            state = env._next_observation()
            #get action
            actions = []
            for agent in agents:
              actions.append(agent.get_action(state).item())
            avg = sum(actions) / len(actions)
            
            # print(f"The scores each agents gave was {actions} with the average of {avg}\n")
            next_state, reward, done, _ = env.step(actions)

            #see if the score they gave is low or high and record
            if avg < lowest_score or lowest_score == False:
              lowest_image = state
              lowest_score = avg
            if avg > highest_score or highest_score == False:
              highest_image = state
              highest_score = avg

            #save it to the replay buffer
            for idx, agent in enumerate(agents):
              agent.replay_buffer.push(state, actions[idx], reward, next_state, done)

            #sum of the rewards in each step
            # print(f"reward in the step {step} is {reward}\n")
            episode_reward += reward

            if len(agents[0].replay_buffer) > batch_size:
              for agent in agents:
                agent.update(batch_size)  

            if done or step == max_steps-1:
                episode_rewards.append(episode_reward)
                print("Episode " + str(episode) + ": " + str(episode_reward) + "\n")
                break

            state = next_state
        
        env.reset()

        plt.imshow(highest_image.permute(1,2,0)/255)
        plt.show()
        plt.imshow(lowest_image.permute(1,2,0)/255)
        plt.show()

    return episode_rewards

import h5py
FilePath = "/content/drive/MyDrive/Marvin/Thesis/dataset/fashiongen_256_256_train.h5"

env = VoteEnv(FilePath)

#SAC 2018 Params
tau = 0.005
gamma = 0.99
value_lr = 3e-3
q_lr = 3e-3
policy_lr = 3e-3
buffer_maxlen = 1000000

agents

agent1 = SACAgent(env, gamma, tau, value_lr, q_lr, policy_lr, buffer_maxlen)
agent2 = SACAgent(env, gamma, tau, value_lr, q_lr, policy_lr, buffer_maxlen)
agent3 = SACAgent(env, gamma, tau, value_lr, q_lr, policy_lr, buffer_maxlen)


agents = [agent1, agent2, agent3]

episode_rewards = mini_batch_train(env, agents, max_episodes=50, max_steps=30, batch_size=30)